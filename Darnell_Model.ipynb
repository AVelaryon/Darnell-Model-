{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7751c434-4976-4e55-b2f7-c34c918eb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "import scipy.stats.qmc as ssq\n",
    "from scipy.stats import chi2, truncnorm\n",
    "from seaborn import displot\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space.space import Real, Integer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer\n",
    "\n",
    "np.set_printoptions(threshold=100_000)\n",
    "X = pd.read_csv(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\ensemble_output\\results\\default\\parameters.csv\")\n",
    "# X = X[['gamma_g', 't_peak', 'gamma_d', 'sd_antarctic',\n",
    "#        'rho_greenland', 'rho_gmsl', 'temperature_0', 'ocean_heat_0', 'Q10',\n",
    "#        'CO2_diffusivity', 'heat_diffusivity', 'rf_scale_aerosol',\n",
    "#        'climate_sensitivity', 'thermal_alpha', 'greenland_a', 'greenland_b',\n",
    "#        'greenland_alpha', 'greenland_beta', 'anto_alpha', 'antarctic_mu',\n",
    "#        'antarctic_precip0', 'antarctic_runoff_height0', 'antarctic_lambda',\n",
    "#        'antarctic_temp_threshold', 'lw_random_sample']].copy()\n",
    "Y = pd.read_csv(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\ensemble_output\\results\\default\\gmslr.csv\")\n",
    "Y = Y.mean(axis=1).rename('output')\n",
    "# X.t_peak = X.t_peak.astype('category')\n",
    "obj = X.join(Y, how='left')\n",
    "# obj.sort_values(by='t_peak',ascending=True, inplace=True)\n",
    "# Rescaling the dataset\n",
    "# min_max = MinMaxScaler()\n",
    "# min_max.fit(obj.iloc[:,:-1],obj.iloc[:,-1])\n",
    "\n",
    "# obj_scaled = min_max.transform(obj.iloc[:,:-1])\n",
    "# obj_y = obj.iloc[:,-1].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nun = len(X.t_peak.unique())\n",
    "discret = KBinsDiscretizer(n_bins=nun, encode='ordinal', random_state=0)\n",
    "Y_binned = discret.fit_transform(Y.to_numpy().reshape(-1,1)).ravel()\n",
    "smote = SMOTENC(categorical_features=['t_peak'],sampling_strategy='auto', random_state=0)\n",
    "X_res , y_res = smote.fit_resample(X, Y_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = pd.Series(discret.inverse_transform(y_res.reshape(-1,1)).ravel(), name='output')\n",
    "obj1 = X_res.join(Y1, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1.sort_values(by='t_peak', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=55, max_features=47, n_estimators=295,\n",
       "                      n_jobs=20, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=55, max_features=47, n_estimators=295,\n",
       "                      n_jobs=20, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=55, max_features=47, n_estimators=295,\n",
       "                      n_jobs=20, random_state=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine model\n",
    "val = train_test_split(obj3060.iloc[:,:-1],obj3060.iloc[:,-1], train_size=0.8, random_state=0)\n",
    "# val = TimeSeriesSplit(n_splits=4, max_train_size=60_066)\n",
    "#Darnell MSE8 n_estimators=199, max_depth=26, max_features=30, min_samples_leaf=2,n_jobs=10,\n",
    "# Darnell MSE10:max_depth=36, max_features=33, n_estimators=300,\n",
    "# max_depth=38, max_features=17,n_estimators=300\n",
    "#==================================================================================================\n",
    "RF = RandomForestRegressor(n_estimators=295, max_features=47, max_depth=55, n_jobs=20,random_state=0)\n",
    "RF.fit(val[0].to_numpy(), val[2].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0076261 , 0.00789796, 0.00783383, 0.00856189, 0.00934584,\n",
       "       0.00818608, 0.00783684, 0.00802969, 0.00736885, 0.00975159,\n",
       "       0.0076832 , 0.00810338, 0.00798281, 0.00829745, 0.00827155,\n",
       "       0.0081994 , 0.00784355, 0.00797682, 0.0079825 , 0.00753266,\n",
       "       0.00786261, 0.01665105, 0.00772726, 0.00857783, 0.00745177,\n",
       "       0.0236806 , 0.29376464, 0.00977019, 0.01184858, 0.01363464,\n",
       "       0.00963379, 0.01521749, 0.00666717, 0.00868914, 0.00872477,\n",
       "       0.0074555 , 0.00784829, 0.00773902, 0.01599659, 0.00783756,\n",
       "       0.0091054 , 0.00804184, 0.00851691, 0.0106149 , 0.007976  ,\n",
       "       0.00799718, 0.00715169, 0.13858481, 0.14292079])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = train_test_split(obj1.iloc[:,:-1],obj1.iloc[:,-1],shuffle=False, train_size=0.8, random_state=0)\n",
    "# val = TimeSeriesSplit(n_splits=4, max_train_size=60_066)\n",
    "#Darnell MSE8 n_estimators=199, max_depth=26, max_features=30, min_samples_leaf=2,n_jobs=10,\n",
    "# Darnell MSE10:max_depth=36, max_features=33, n_estimators=300,\n",
    "# max_depth=38, max_features=17,n_estimators=300\n",
    "#==================================================================================================\n",
    "RF = RandomForestRegressor(n_estimators=295, max_features=47, max_depth=55, n_jobs=20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.fit(val[0], val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train MSE={np.mean(np.square(val[2].to_numpy()-RF.predict(val[0].to_numpy())))}')\n",
    "print(f'Test MSE={np.mean(np.square(val[3].to_numpy()-RF.predict(val[1].to_numpy())))}')\n",
    "# print(f'Importances:\\n{RF.feature_importances_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = dict()\n",
    "parameter_space['n_estimators'] = Integer(30,300)\n",
    "# parameter_space['min_samples_split'] = Integer(2,5, prior='uniform')\n",
    "# parameter_space['min_samples_leaf'] = Integer(1,5)\n",
    "parameter_space['max_features'] = Integer(10,53)\n",
    "parameter_space['max_depth'] = Integer(10,80)\n",
    "# parameter_space['ccp_alpha'] = Real(1e-4, 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bae = BayesSearchCV(estimator=RF,search_spaces=parameter_space,verbose=2, n_iter=100, scoring='neg_mean_squared_error', n_points=6, cv=val, n_jobs=24, random_state=0)\n",
    "bae.fit(obj1.iloc[:,:-1], obj1.iloc[:,-1])\n",
    "res = pd.DataFrame(bae.cv_results_)\n",
    "res.to_excel(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\Hyperparameter_Tuning\\Darnell_HT_MSE_12.xlsx\")\n",
    "res.to_csv(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\Hyperparameter_Tuning\\Darnell_HT_MSE_12.csv\")\n",
    "print(f'Best Parameters:{bae.best_params_} with MSE: {bae.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=0)\n",
    "def bootstrapp(y_true,y_pred,n_boots):\n",
    "    rows = len(y_true)\n",
    "    error = (y_true - y_pred).to_numpy()\n",
    "    S_CI = np.zeros((n_boots,))\n",
    "    for i in range(n_boots):\n",
    "        indx = rng.choice(rows, size=rows, replace=True)\n",
    "        S_CI[i] = np.mean(np.square(error[indx]))\n",
    "    return S_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvi1(data: pd.DataFrame, y_true: np.ndarray[float], f: callable,*, n_boots: int, alpha: float, column_set: dict) -> tuple[pd.DataFrame]:\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = column_set\n",
    "    V_y = np.var(y_true, ddof=1)\n",
    "    u = np.zeros((columns,))\n",
    "    CI = dict()\n",
    "    # rolled_data = pd.DataFrame(np.roll(data.copy(),rows//2, axis=0), columns=data.columns)\n",
    "    rolled_data = np.roll(data.copy(),rows//2, axis=0)\n",
    "    # for cat,col in npv_columns.items():\n",
    "    for col in npv_columns:\n",
    "        d = rolled_data.copy()\n",
    "        d[:, col] = data[:,col]\n",
    "        y_pred = f(d)\n",
    "        u[col] = 1-np.mean(np.square(y_true - y_pred))/(2*V_y)\n",
    "    #     S_CI = 1 - bootstrapp(y_true,y_pred, n_boots)/(2*V_y)\n",
    "    #     p0,p1 = np.quantile(S_CI, [alpha,1-alpha])\n",
    "    #     CI[col] = [p0,p1,p1-p0]\n",
    "    # CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    # CI.index.name = 'Interactions'\n",
    "    # CI.columns.name = 'Confidence Interval Difference'\n",
    "    # SI = pd.DataFrame(u.values(), index=u.keys(), columns=['$P_{i}$'])\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_pvi1(data: pd.DataFrame, y_true: np.ndarray[float], f: callable,*, n_boots: int, alpha: float, column_set: dict) -> tuple[pd.DataFrame]:\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = column_set\n",
    "    V_y = np.var(y_true, ddof=1)\n",
    "    u = dict()\n",
    "    CI = dict()\n",
    "    rolled_data = pd.DataFrame(np.roll(data.copy(),rows//2, axis=0), columns=data.columns)\n",
    "    # rolled_data = np.roll(data.copy(), rows//2, axis=0)\n",
    "    for cat, col in npv_columns.items():\n",
    "        d = rolled_data.copy()\n",
    "        d[col] = data[col].to_numpy()\n",
    "        y_pred = f(d)\n",
    "        u[cat] = 1-np.mean(np.square(y_true - y_pred))/(2*V_y)\n",
    "        S_CI = 1 - bootstrapp(y_true,y_pred, n_boots)/(2*V_y)\n",
    "        p0,p1 = np.quantile(S_CI, [alpha,1-alpha])\n",
    "        CI[cat] = [p0,p1,p1-p0]\n",
    "    CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    CI.index.name = 'Interactions'\n",
    "    CI.columns.name = 'Confidence Interval Difference'\n",
    "    SI = pd.DataFrame(u.values(), index=u.keys(), columns=['$P_{i}$'])\n",
    "    return u,CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvi_t(data: np.ndarray[float], y_true: np.ndarray[float], f: callable,*,n_boots: int, alpha: float, column_set: list[str]) -> np.ndarray[float]:\n",
    "    '''\n",
    "    ===Total-Order Permutation Variable Importances===\n",
    "\n",
    "        data: your training dataset\n",
    "        y_pred: your model output (specifically the prediction of your 'data' arg)\n",
    "        f: in this case, simply RF.predict\n",
    "    '''\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = column_set\n",
    "    u = np.zeros((columns,))\n",
    "    V_y = np.var(y_true, ddof=1)\n",
    "    CI = dict()\n",
    "    for col in npv_columns:\n",
    "        d = data.copy()\n",
    "        d[:, col] = np.roll(data[:, col],rows//2, axis=0)\n",
    "        y_pred = f(d)\n",
    "        u[col] = np.mean(np.square(y_true - y_pred))/(2*V_y)\n",
    "    #     S_CI = bootstrapp(y_true,y_pred, n_boots)/(2*V_y)\n",
    "    #     p0,p1 = np.quantile(S_CI, [alpha,1-alpha])\n",
    "    #     CI[col] = [p0,p1,p1-p0]\n",
    "    # CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    # CI.index.name = 'Interactions'\n",
    "    # CI.columns.name = 'Confidence Interval Difference'\n",
    "    # S_t = pd.DataFrame(u.values(), index=u.keys(), columns=[r'$P_{\\tau}$'])\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_pvi_t(data: np.ndarray[float], y_true: np.ndarray[float], f: callable,*,n_boots: int, alpha: float, column_set: list[str]) -> np.ndarray[float]:\n",
    "    '''\n",
    "    ===Total-Order Permutation Variable Importances===\n",
    "\n",
    "        data: your training dataset\n",
    "        y_pred: your model output (specifically the prediction of your 'data' arg)\n",
    "        f: in this case, simply RF.predict\n",
    "    '''\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = column_set\n",
    "    u = dict()\n",
    "    V_y = np.var(y_true,ddof=1 )\n",
    "    CI = dict()\n",
    "    for cat,col in npv_columns.items():\n",
    "        d = data.copy()\n",
    "        d[col] = np.roll(data[col],rows//2, axis=0)\n",
    "        y_pred = f(d)\n",
    "        u[cat] = np.mean(np.square(y_true - y_pred))/(2*V_y)\n",
    "        S_CI = bootstrapp(y_true,y_pred, n_boots)/(2*V_y)\n",
    "        p0,p1 = np.quantile(S_CI, [alpha,1-alpha])\n",
    "        CI[cat] = [p0,p1,p1-p0]\n",
    "    CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    CI.index.name = 'Interactions'\n",
    "    CI.columns.name = 'Confidence Interval Difference'\n",
    "    S_t = pd.DataFrame(u.values(), index=u.keys(), columns=[r'$P_{\\tau}$'])\n",
    "    return u, CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvi2(data: np.ndarray[float], y_true: np.ndarray[float], f: callable,*,S_I: np.ndarray[float] ,n_boots: int, alpha: float, columns_set: dict) -> tuple[np.ndarray[float], list]:\n",
    "    '''\n",
    "    ===Second-Order Permutation Variable Importances (Full Matrix)===\n",
    "\n",
    "        data: your training dataset\n",
    "        y_pred: your model output (specifically the y_true value)\n",
    "        f: in this case, simply RF.predict\n",
    "\n",
    "        S_I: First-Order Importances\n",
    "        n_boots: number of bootstrap samples\n",
    "        alpha: parameter for confidence interval [alpha, 1-alpha]\n",
    "        S_CI: sampling distribution \n",
    "        CI: Confidence Intervals\n",
    "    '''\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = columns_set\n",
    "    n = n_boots\n",
    "    S = np.zeros((columns, columns))\n",
    "    CI = dict()\n",
    "    V_y = np.var(y_true, ddof=1)\n",
    "    # rolled_data = pd.DataFrame(np.roll(data.copy(),rows//2, axis=0), columns=data.columns)\n",
    "    rolled_data = np.roll(data.copy(),rows//2, axis=0)\n",
    "    for col1,col2 in it.combinations(npv_columns, r=2):\n",
    "        d = rolled_data.copy()\n",
    "        d[:,[col1, col2]] = data[:,[col1,col2]]\n",
    "        y_pred = f(d)\n",
    "        S_cat1 = S_I[col1]\n",
    "        S_cat2 = S_I[col2]\n",
    "        print(f'First-Order:\\t{S_cat1, S_cat2}')\n",
    "        # print(f'First Part:\\t{np.mean(np.square(y_true - f(d)))/(2*V_y)}')\n",
    "        # print(f'Second Part:\\t{S_cat1}')\n",
    "        # print(f'Third Part:\\t{S_cat2}')\n",
    "        S[col1, col2] = 1 - np.mean(np.square(y_true - y_pred))/(2*V_y) - S_cat1 - S_cat2\n",
    "        print(f'Second-Order:\\t{S[col1,col2]}')\n",
    "\n",
    "    #     S_CI = 1 - bootstrapp(y_true,y_pred, n_boots)/(2*V_y) - S_cat1 - S_cat2\n",
    "    #     p0,p1= np.quantile(S_CI, [alpha,1-alpha])\n",
    "    #     CI[(col1, col2)] = [p0,p1,p1-p0]\n",
    "    # CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    # CI.index.name = 'Interactions'\n",
    "    # CI.columns.name = 'Confidence Interval Difference'\n",
    "\n",
    "    # S = pd.DataFrame(S.values(), index=S.keys(), columns=[r'$P_{ik}$'])\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_pvi2(data: np.ndarray[float], y_true: np.ndarray[float], f: callable,*,S_I: np.ndarray[float] ,n_boots: int, alpha: float, columns_set: dict) -> tuple[np.ndarray[float], list]:\n",
    "    '''\n",
    "    ===Second-Order Permutation Variable Importances (Full Matrix)===\n",
    "\n",
    "        data: your training dataset\n",
    "        y_pred: your model output (specifically the y_true value)\n",
    "        f: in this case, simply RF.predict\n",
    "\n",
    "        S_I: First-Order Importances\n",
    "        n_boots: number of bootstrap samples\n",
    "        alpha: parameter for confidence interval [alpha, 1-alpha]\n",
    "        S_CI: sampling distribution \n",
    "        CI: Confidence Intervals\n",
    "    '''\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = columns_set\n",
    "    n = n_boots\n",
    "    S = dict()\n",
    "    CI = dict()\n",
    "    V_y = np.var(y_true, ddof=1)\n",
    "    rolled_data = pd.DataFrame(np.roll(data.copy(),rows//2, axis=0), columns=data.columns)\n",
    "    # rolled_data = np.roll(data.copy(), rows//2, axis=0)\n",
    "    for (cat1,col1),(cat2, col2) in it.combinations(npv_columns.items(), r=2):\n",
    "        col = col1+col2\n",
    "        d = rolled_data.copy()\n",
    "        d[col] = data[col].to_numpy()\n",
    "        y_pred = f(d)\n",
    "        S_cat1 = S_I[cat1]\n",
    "        S_cat2 = S_I[cat2]\n",
    "        S[(cat1, cat2)] = 1 - np.mean(np.square(y_true - y_pred))/(2*V_y) - S_cat1 - S_cat2\n",
    "        S_CI = 1 - bootstrapp(y_true,y_pred, n_boots)/(2*V_y) - S_cat1 - S_cat2 # Bootstrap Process\n",
    "        p0,p1= np.quantile(S_CI, [alpha,1-alpha])\n",
    "        CI[(cat1, cat2)] = [p0,p1,p1-p0]\n",
    "    CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    CI.index.name = 'Interactions'\n",
    "    CI.columns.name = 'Confidence Interval Difference'\n",
    "\n",
    "    # S = pd.DataFrame(S.values(), index=S.keys(), columns=[r'$P_{ik}$'])\n",
    "    return S, CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variables = ['sd_temp', 'sd_ocean_heat', 'sd_glaciers', 'sd_greenland','sd_antarctic',\n",
    "              'sd_gmsl', 'rho_temperature','rho_ocean_heat', \n",
    "              'rho_glaciers', 'rho_greenland', 'rho_antarctic','rho_gmsl']\n",
    "Climate = ['CO2_0', 'N2O_0', 'temperature_0','ocean_heat_0','Q10', 'CO2_fertilization',\n",
    "           'CO2_diffusivity', 'heat_diffusivity', 'rf_scale_aerosol','climate_sensitivity']\n",
    "Glaciers = ['glaciers_v0','glaciers_s0','glaciers_beta0', 'glaciers_n']\n",
    "Thermals = ['thermal_s0', 'thermal_alpha']\n",
    "\n",
    "Greenland = ['greenland_a', 'greenland_b','greenland_alpha', 'greenland_beta']\n",
    "Antarctic = [ 'antarctic_gamma', 'antarctic_alpha','antarctic_mu', 'antarctic_nu',\n",
    "              'antarctic_precip0', 'antarctic_kappa','antarctic_flow0', 'antarctic_runoff_height0',\n",
    "                'antarctic_c','antarctic_bed_height0', 'antarctic_slope', 'antarctic_lambda','antarctic_temp_threshold',\n",
    "                'antarctic_s0','anto_alpha', 'anto_beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_set = val[0].columns\n",
    "# print(column_set)\n",
    "npv_column_set = dict()\n",
    "npv_column_set['Variables'] = Variables\n",
    "npv_column_set['Climate'] = Climate\n",
    "# npv_column_set['Glaciers'] = Glaciers\n",
    "npv_column_set['Thermals'] = Thermals\n",
    "npv_column_set['Greenland'] = Greenland\n",
    "npv_column_set['Antarctic'] = Antarctic\n",
    "npv_column_set['Emissions'] = ['gamma_g', 't_peak', 'gamma_d', 'lw_random_sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-Order:\t(0.30283277896206695, 0.3031694628315802)\n",
      "Second-Order:\t-0.29885630487558346\n",
      "First-Order:\t(0.30283277896206695, 0.3030293066041312)\n",
      "Second-Order:\t-0.2988369776497797\n",
      "First-Order:\t(0.30283277896206695, 0.3046638327562353)\n",
      "Second-Order:\t-0.2989073211289537\n",
      "First-Order:\t(0.30283277896206695, 0.3047757892636195)\n",
      "Second-Order:\t-0.2989403280063302\n",
      "First-Order:\t(0.30283277896206695, 0.3036274909992027)\n",
      "Second-Order:\t-0.29886141773328245\n",
      "First-Order:\t(0.30283277896206695, 0.3032741055314556)\n",
      "Second-Order:\t-0.2988548233471413\n",
      "First-Order:\t(0.30283277896206695, 0.3030375106314571)\n",
      "Second-Order:\t-0.2988445294244241\n",
      "First-Order:\t(0.30283277896206695, 0.3028086166954488)\n",
      "Second-Order:\t-0.29883297162858447\n",
      "First-Order:\t(0.30283277896206695, 0.3064823943427514)\n",
      "Second-Order:\t-0.29897624816334545\n",
      "First-Order:\t(0.30283277896206695, 0.30294877192612113)\n",
      "Second-Order:\t-0.2988372549691566\n",
      "First-Order:\t(0.30283277896206695, 0.30399132314041644)\n",
      "Second-Order:\t-0.29889528556481415\n",
      "First-Order:\t(0.30283277896206695, 0.30300120736262703)\n",
      "Second-Order:\t-0.2988766996136646\n",
      "First-Order:\t(0.30283277896206695, 0.3033353180956577)\n",
      "Second-Order:\t-0.29885875691476804\n",
      "First-Order:\t(0.30283277896206695, 0.30688344704440007)\n",
      "Second-Order:\t-0.29891583182689496\n",
      "First-Order:\t(0.30283277896206695, 0.30554160177782097)\n",
      "Second-Order:\t-0.2988945700861302\n",
      "First-Order:\t(0.30283277896206695, 0.3028811913659576)\n",
      "Second-Order:\t-0.29884386378172423\n",
      "First-Order:\t(0.30283277896206695, 0.30329673691229797)\n",
      "Second-Order:\t-0.29885719402222166\n",
      "First-Order:\t(0.30283277896206695, 0.3034206473186253)\n",
      "Second-Order:\t-0.29886692076814525\n",
      "First-Order:\t(0.30283277896206695, 0.3029355813282687)\n",
      "Second-Order:\t-0.2988331338579969\n",
      "First-Order:\t(0.30283277896206695, 0.30313233676782947)\n",
      "Second-Order:\t-0.29886550115622945\n",
      "First-Order:\t(0.30283277896206695, 0.31151937840783084)\n",
      "Second-Order:\t-0.29941852263500346\n",
      "First-Order:\t(0.30283277896206695, 0.3031785422472064)\n",
      "Second-Order:\t-0.29883947143488876\n",
      "First-Order:\t(0.30283277896206695, 0.30390204981565005)\n",
      "Second-Order:\t-0.29889083507135394\n",
      "First-Order:\t(0.30283277896206695, 0.30272100440020466)\n",
      "Second-Order:\t-0.2988345629343201\n",
      "First-Order:\t(0.30283277896206695, 0.3391662198635065)\n",
      "Second-Order:\t-0.2996476815214172\n",
      "First-Order:\t(0.30283277896206695, 0.4971344775732859)\n",
      "Second-Order:\t-0.30213113891066723\n",
      "First-Order:\t(0.30283277896206695, 0.30256943096888567)\n",
      "Second-Order:\t-0.29898346627031147\n",
      "First-Order:\t(0.30283277896206695, 0.30903310929395145)\n",
      "Second-Order:\t-0.2990814376707106\n",
      "First-Order:\t(0.30283277896206695, 0.3117736214808978)\n",
      "Second-Order:\t-0.2993089160290854\n",
      "First-Order:\t(0.30283277896206695, 0.3049120918769219)\n",
      "Second-Order:\t-0.29896191224919844\n",
      "First-Order:\t(0.30283277896206695, 0.3150327935842946)\n",
      "Second-Order:\t-0.29939668790340723\n",
      "First-Order:\t(0.30283277896206695, 0.30270726946422133)\n",
      "Second-Order:\t-0.2988306017191892\n",
      "First-Order:\t(0.30283277896206695, 0.303922838401134)\n",
      "Second-Order:\t-0.2988772898149279\n",
      "First-Order:\t(0.30283277896206695, 0.3043172121403891)\n",
      "Second-Order:\t-0.29891457196353255\n",
      "First-Order:\t(0.30283277896206695, 0.3027055156884704)\n",
      "Second-Order:\t-0.298848909085876\n",
      "First-Order:\t(0.30283277896206695, 0.30312932896829825)\n",
      "Second-Order:\t-0.2988528684310586\n",
      "First-Order:\t(0.30283277896206695, 0.30327446196157615)\n",
      "Second-Order:\t-0.29886256752918283\n",
      "First-Order:\t(0.30283277896206695, 0.3137076999013675)\n",
      "Second-Order:\t-0.29925058311307196\n",
      "First-Order:\t(0.30283277896206695, 0.3033168609304514)\n",
      "Second-Order:\t-0.2988491290874309\n",
      "First-Order:\t(0.30283277896206695, 0.306526483280411)\n",
      "Second-Order:\t-0.2989825677849085\n",
      "First-Order:\t(0.30283277896206695, 0.3032959178171698)\n",
      "Second-Order:\t-0.29885424499076674\n",
      "First-Order:\t(0.30283277896206695, 0.30414572828617126)\n",
      "Second-Order:\t-0.2989055142808491\n",
      "First-Order:\t(0.30283277896206695, 0.30573158647934995)\n",
      "Second-Order:\t-0.29897100107213737\n",
      "First-Order:\t(0.30283277896206695, 0.3034340075813158)\n",
      "Second-Order:\t-0.2988774287920313\n",
      "First-Order:\t(0.30283277896206695, 0.30307829372100503)\n",
      "Second-Order:\t-0.2988574584122091\n",
      "First-Order:\t(0.30283277896206695, 0.3028913736501615)\n",
      "Second-Order:\t-0.2988526661533304\n",
      "First-Order:\t(0.30283277896206695, 0.4007799130675139)\n",
      "Second-Order:\t-0.3015955230138426\n",
      "First-Order:\t(0.30283277896206695, 0.4121820212132179)\n",
      "Second-Order:\t-0.3016886883127161\n",
      "First-Order:\t(0.3031694628315802, 0.3030293066041312)\n",
      "Second-Order:\t-0.29888046410284597\n",
      "First-Order:\t(0.3031694628315802, 0.3046638327562353)\n",
      "Second-Order:\t-0.29894903032456965\n",
      "First-Order:\t(0.3031694628315802, 0.3047757892636195)\n",
      "Second-Order:\t-0.29895554839461613\n",
      "First-Order:\t(0.3031694628315802, 0.3036274909992027)\n",
      "Second-Order:\t-0.29891626347244393\n",
      "First-Order:\t(0.3031694628315802, 0.3032741055314556)\n",
      "Second-Order:\t-0.2988862274308475\n",
      "First-Order:\t(0.3031694628315802, 0.3030375106314571)\n",
      "Second-Order:\t-0.2988936477611125\n",
      "First-Order:\t(0.3031694628315802, 0.3028086166954488)\n",
      "Second-Order:\t-0.2988414699933011\n",
      "First-Order:\t(0.3031694628315802, 0.3064823943427514)\n",
      "Second-Order:\t-0.2989790524473511\n",
      "First-Order:\t(0.3031694628315802, 0.30294877192612113)\n",
      "Second-Order:\t-0.29886139227406905\n",
      "First-Order:\t(0.3031694628315802, 0.30399132314041644)\n",
      "Second-Order:\t-0.2989556855597292\n",
      "First-Order:\t(0.3031694628315802, 0.30300120736262703)\n",
      "Second-Order:\t-0.29889663622868246\n",
      "First-Order:\t(0.3031694628315802, 0.3033353180956577)\n",
      "Second-Order:\t-0.2988916724647165\n",
      "First-Order:\t(0.3031694628315802, 0.30688344704440007)\n",
      "Second-Order:\t-0.2989458894549497\n",
      "First-Order:\t(0.3031694628315802, 0.30554160177782097)\n",
      "Second-Order:\t-0.29895486861841725\n",
      "First-Order:\t(0.3031694628315802, 0.3028811913659576)\n",
      "Second-Order:\t-0.29887908688202036\n",
      "First-Order:\t(0.3031694628315802, 0.30329673691229797)\n",
      "Second-Order:\t-0.29888730690909926\n",
      "First-Order:\t(0.3031694628315802, 0.3034206473186253)\n",
      "Second-Order:\t-0.2989056589096102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m PI\u001b[38;5;241m=\u001b[39m pvi1(val[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy(), val[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy(), RF\u001b[38;5;241m.\u001b[39mpredict, n_boots\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, column_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumns)))\n\u001b[1;32m----> 2\u001b[0m PIk, CIk\u001b[38;5;241m=\u001b[39m \u001b[43mpvi2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mS_I\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_boots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(PI)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(PIk)\n",
      "Cell \u001b[1;32mIn[29], line 26\u001b[0m, in \u001b[0;36mpvi2\u001b[1;34m(data, y_true, f, S_I, n_boots, alpha, columns_set)\u001b[0m\n\u001b[0;32m     24\u001b[0m d \u001b[38;5;241m=\u001b[39m rolled_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     25\u001b[0m d[:,[col1, col2]] \u001b[38;5;241m=\u001b[39m data[:,[col1,col2]]\n\u001b[1;32m---> 26\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m S_cat1 \u001b[38;5;241m=\u001b[39m S_I[col1]\n\u001b[0;32m     28\u001b[0m S_cat2 \u001b[38;5;241m=\u001b[39m S_I[col2]\n",
      "File \u001b[1;32mc:\\Users\\dwigh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:997\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;66;03m# Parallel loop\u001b[39;00m\n\u001b[0;32m    996\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[1;32m--> 997\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharedmem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m y_hat \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "File \u001b[1;32mc:\\Users\\dwigh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dwigh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dwigh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dwigh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PI= pvi1(val[0].to_numpy(), val[2].to_numpy(), RF.predict, n_boots=1000, alpha=0.05, column_set=range(len(val[0].columns)))\n",
    "PIk, CIk= pvi2(val[0].to_numpy(), val[2].to_numpy(), RF.predict,S_I=PI, n_boots=1000, alpha=0.05, columns_set=range(len(val[0].columns)))\n",
    "print(PI)\n",
    "print(PIk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donut_chart(values: np.ndarray, values1: np.ndarray, labels: list,labels1: list, props: dict, kw: dict):\n",
    "    fig, ax= plt.subplots(1,2,dpi=500, figsize=(20,25))\n",
    "    wedges0, text0 = ax[0].pie(values,wedgeprops=props['wedgeprops'],colors=props['colors'], startangle= props['startangle'], textprops = props['textprops'])\n",
    "    for i, p in enumerate(wedges0):\n",
    "        ang = (p.theta2-p.theta1)/2 + p.theta1\n",
    "        x,y = np.cos(np.deg2rad(ang)), np.sin(np.deg2rad(ang))\n",
    "        horizontalalignment = {-1:'right', 1:'left'}[int(np.sign(x))]\n",
    "        connectionstyle = f'angle, angleA=0, angleB={ang}'\n",
    "        kw['arrowprops'].update({'connectionstyle':connectionstyle})\n",
    "        ax[0].annotate(f'{values[i]: .1%}', xy=(x,y), xytext=(1.1*np.sign(x),1.2*y), horizontalalignment=horizontalalignment, **kw)\n",
    "    ax[0].set_title('SNEASY-BRICK: First & Second-Order PVi')\n",
    "    ax[0].legend(labels=labels,loc='center', title='PVi Indices:' )\n",
    "\n",
    "    wedges1, text1 = ax[1].pie(values1,wedgeprops=props['wedgeprops'],colors=props['colors'], startangle= props['startangle'], textprops = props['textprops'])\n",
    "    for i, p in enumerate(wedges1):\n",
    "        ang = (p.theta2-p.theta1)/2 + p.theta1\n",
    "        x,y = np.cos(np.deg2rad(ang)), np.sin(np.deg2rad(ang))\n",
    "        horizontalalignment = {-1:'right', 1:'left'}[int(np.sign(x))]\n",
    "        connectionstyle = f'angle, angleA=0, angleB={ang}'\n",
    "        kw['arrowprops'].update({'connectionstyle':connectionstyle})\n",
    "        ax[1].annotate(f'{values1[i]: .1%}', xy=(x,y), xytext=(1.1*np.sign(x),1.2*y), horizontalalignment=horizontalalignment, **kw)\n",
    "    ax[1].set_title('SNEASY-BRICK: Total-Order PVi')\n",
    "    ax[1].legend(labels=labels1,loc='center', title='PVi Indices:' )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Variables': 0.015095430276762934,\n",
       "  'Climate': 0.32375627783499483,\n",
       "  'Thermals': 0.008758090734658521,\n",
       "  'Greenland': 0.014086980856964519,\n",
       "  'Antarctic': 0.2526646463020993,\n",
       "  'Emissions': 0.5727934689329497},\n",
       " Confidence Interval Difference       5th      95th  Quantile Difference\n",
       " Interactions                                                           \n",
       " Variables                       0.014623  0.015590             0.000967\n",
       " Climate                         0.315944  0.332229             0.016285\n",
       " Thermals                        0.008467  0.009045             0.000578\n",
       " Greenland                       0.013752  0.014454             0.000702\n",
       " Antarctic                       0.245806  0.259209             0.013403\n",
       " Emissions                       0.562302  0.583772             0.021471)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT= mod_pvi_t(val[0], val[2], RF.predict, n_boots=1000, alpha=0.05, column_set=npv_column_set)\n",
    "PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIK,CIK= mod_pvi2(val[0], val[2], RF.predict,S_I=PI, n_boots=1000, alpha=0.05, columns_set=npv_column_set)\n",
    "PIK = pd.DataFrame(PIK.values(), index=PIK.keys(), columns=[r'$P_{ik}$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>$P_{ik}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Variables</th>\n",
       "      <th>Climate</th>\n",
       "      <td>-0.051538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thermals</th>\n",
       "      <td>-0.035164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greenland</th>\n",
       "      <td>-0.037669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antarctic</th>\n",
       "      <td>-0.052738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emissions</th>\n",
       "      <td>-0.050539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Climate</th>\n",
       "      <th>Thermals</th>\n",
       "      <td>-0.035737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greenland</th>\n",
       "      <td>-0.041018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antarctic</th>\n",
       "      <td>-0.055184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emissions</th>\n",
       "      <td>-0.050734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Thermals</th>\n",
       "      <th>Greenland</th>\n",
       "      <td>-0.034807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antarctic</th>\n",
       "      <td>-0.037930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emissions</th>\n",
       "      <td>-0.037399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Greenland</th>\n",
       "      <th>Antarctic</th>\n",
       "      <td>-0.044441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emissions</th>\n",
       "      <td>-0.043071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antarctic</th>\n",
       "      <th>Emissions</th>\n",
       "      <td>-0.030327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     $P_{ik}$\n",
       "Variables Climate   -0.051538\n",
       "          Thermals  -0.035164\n",
       "          Greenland -0.037669\n",
       "          Antarctic -0.052738\n",
       "          Emissions -0.050539\n",
       "Climate   Thermals  -0.035737\n",
       "          Greenland -0.041018\n",
       "          Antarctic -0.055184\n",
       "          Emissions -0.050734\n",
       "Thermals  Greenland -0.034807\n",
       "          Antarctic -0.037930\n",
       "          Emissions -0.037399\n",
       "Greenland Antarctic -0.044441\n",
       "          Emissions -0.043071\n",
       "Antarctic Emissions -0.030327"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(PI.values(), index=PI.keys()).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = dict()\n",
    "kw = dict()\n",
    "props['textprops'] = {'fontsize':14}\n",
    "props['startangle'] = -270\n",
    "props['wedgeprops'] = {'width': 0.2}\n",
    "kw['arrowprops'] = dict(arrowstyle='-')\n",
    "kw['zorder'] = 0\n",
    "kw['va'] = 'center'\n",
    "colors = ['#CF0057','#FF0054','#F72585', '#B5179E', '#7209B7', '#560BAD','#8F3DC1','#A96FD1',\n",
    "          '#C3A3E0', '#8F6FD1', '#480CA8', '#3A0CA3','#141440','#A3A3E0', '#6F6FD1', '#3D3DC1',\n",
    "          '#4361EE','#4CC9F0','#1DE48D','#6B007B','#B51CE4','#E044A7', '#744EC2']\n",
    "props['colors'] = colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = pd.DataFrame(PI.values(), index=PI.keys(), columns=[r'$P_{i}$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = PI.to_dict()[r'$P_{i}$']\n",
    "inner = {key: value for key, value in inner.items() if value>0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_inner = PIK.to_dict()[r'$P_{ik}$']\n",
    "left_inner = {','.join(key): value for key,value in left_inner.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_indices = inner|left_inner\n",
    "labels = list(merged_indices.keys())\n",
    "values = list(merged_indices.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT, CT = PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = list(PT.keys())\n",
    "values1 = list(PT.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donut_chart(values, values1, labels, labels1, props, kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1,1,dpi=500, figsize=(8,8))\n",
    "wedges0, text0 = ax.pie(values,wedgeprops=props['wedgeprops'],colors=props['colors'], startangle= props['startangle'], textprops = props['textprops'])\n",
    "for i, p in enumerate(wedges0):\n",
    "    ang = (p.theta2-p.theta1)/2 + p.theta1\n",
    "    x,y = np.cos(np.deg2rad(ang)), np.sin(np.deg2rad(ang))\n",
    "    horizontalalignment = {-1:'right', 1:'left'}[int(np.sign(x))]\n",
    "    connectionstyle = f'angle, angleA=0, angleB={ang}'\n",
    "    kw['arrowprops'].update({'connectionstyle':connectionstyle})\n",
    "    ax.annotate(f'{values[i]: .1%}', xy=(x,y), xytext=(1.1*np.sign(x),1.2*y), horizontalalignment=horizontalalignment, **kw)\n",
    "ax.set_title('SNEASY-BRICK: First & Second-Order PVi')\n",
    "ax.legend(labels=labels,loc='center', title='PVi Indices:' , prop={'size':6})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1,1,dpi=250, figsize=(8,8))\n",
    "wedges0, text0 = ax.pie(values1,wedgeprops=props['wedgeprops'],colors=props['colors'], startangle= props['startangle'], textprops = props['textprops'])\n",
    "for i, p in enumerate(wedges0):\n",
    "    ang = (p.theta2-p.theta1)/2 + p.theta1\n",
    "    x,y = np.cos(np.deg2rad(ang)), np.sin(np.deg2rad(ang))\n",
    "    horizontalalignment = {-1:'right', 1:'left'}[int(np.sign(x))]\n",
    "    connectionstyle = f'angle, angleA=0, angleB={ang}'\n",
    "    kw['arrowprops'].update({'connectionstyle':connectionstyle})\n",
    "    ax.annotate(f'{values1[i]: .1%}', xy=(x,y), xytext=(1.1*np.sign(x),1.2*y), horizontalalignment=horizontalalignment, **kw)\n",
    "ax.set_title('SNEASY-BRICK: Total-Order PVi')\n",
    "ax.legend(labels=labels1,loc='center', title='PVi Indices:' , prop={'size':6})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI_CI = PI.join(CI, how='left')\n",
    "print(PI_CI.to_latex(float_format='{:.4f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT = pd.DataFrame(PT.values(), index=PT.keys(), columns=[r'$P_{\\tau}$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_CT = PT.join(CT, how='left')\n",
    "print(PT_CT.to_latex(float_format='{:.4f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIK_CIK = PIK.join(CIK, how='left')\n",
    "print(PIK_CIK.to_latex(float_format='{:.4f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(obj.t_peak, obj.output, color='blue', label='True')\n",
    "plt.plot(obj.t_peak, RF.predict(obj.iloc[:,:-1]), color='firebrick', label='Predicted')\n",
    "plt.title('Test Predictions vs. True Output')\n",
    "plt.ylabel('Risk Estimate')\n",
    "plt.xlabel(r'$\\tau$ (year of peak emissions)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Per Year Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_years = obj['t_peak'].unique() # Obtain the unique years\n",
    "mask = u_years<2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_PI = dict()\n",
    "time_PIK = dict()\n",
    "time_PT = dict()\n",
    "for year in u_years[mask]:\n",
    "    # if year>=2161:\n",
    "    #     pass\n",
    "    # else:\n",
    "    #     # print(year)\n",
    "    mask = obj['t_peak'] < year\n",
    "    val1 = train_test_split(obj[mask].iloc[:,:-1], obj[mask].iloc[:,-1], train_size=0.5, random_state=0, shuffle=False)\n",
    "    RF = RandomForestRegressor(n_jobs=10, random_state=0)\n",
    "    RF.fit(val1[0], val1[2])\n",
    "    time_PI[year] = mod_pvi1(val1[0], val1[2], RF.predict, n_boots=1000, alpha=0.05, column_set=npv_column_set)\n",
    "    time_PT[year] = mod_pvi_t(val1[0], val1[2], RF.predict, n_boots=1000, alpha=0.05, column_set=npv_column_set)\n",
    "    time_PIK[year] = mod_pvi2(val1[0], val1[2], RF.predict,S_I=time_PI[year], n_boots=1000, alpha=0.05, columns_set=npv_column_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(time_PT.values(), index=time_PT.keys())\n",
    "res.to_excel(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\Time_PVI\\tot_time_PVI.xlsx\")\n",
    "res = pd.DataFrame(time_PI.values(), index=time_PI.keys())\n",
    "res.to_excel(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\Time_PVI\\first_time_PVI.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will append a noise feature $S_{\\mathrm{var}}\\sim \\mathcal{N}(\\mathbf{\\overline{0}}, \\sigma^{2})$  to the dataset, to accentuate pertinent features. Thereafter, we select features based upon the total-order PVi contribution of $S_{\\mathrm{var}}$, as all features contributing $\\leqslant$ are equally insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefing the TPVi for compatibility\n",
    "def mpt(data: np.ndarray[float], y_true: np.ndarray[float], f: callable,*,n_boots: int, alpha: float, column_set: list[str]) -> np.ndarray[float]:\n",
    "    '''\n",
    "    ===Total-Order Permutation Variable Importances===\n",
    "\n",
    "        data: your training dataset\n",
    "        y_pred: your model output (specifically the prediction of your 'data' arg)\n",
    "        f: in this case, simply RF.predict\n",
    "    '''\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = column_set\n",
    "    u = dict()\n",
    "    V_y = np.var(y_true,ddof=1 )\n",
    "    CI = dict()\n",
    "    for col in npv_columns:\n",
    "        d = data.copy()\n",
    "        d[col] = np.roll(data[col],rows//2, axis=0)\n",
    "        y_pred = f(d)\n",
    "        u[col] = np.mean(np.square(y_true - y_pred))/(2*V_y)\n",
    "        S_CI = bootstrapp(y_true,y_pred, n_boots)/(2*V_y)\n",
    "        p0,p1 = np.quantile(S_CI, [alpha,1-alpha])\n",
    "        CI[col] = [p0,p1,p1-p0]\n",
    "    CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    CI.index.name = 'Interactions'\n",
    "    CI.columns.name = 'Confidence Interval Difference'\n",
    "    S_t = pd.DataFrame(u.values(), index=u.keys(), columns=[r'$P_{\\tau}$'])\n",
    "    return u, CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obj = obj1.copy()\n",
    "S_var = chi2.rvs(1,loc=0,scale=1, size=obj1.shape[0], random_state=0)\n",
    "new_obj.insert(0, 'S_var',S_var)\n",
    "# new_obj.sort_values(by='t_peak',ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obj.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Fitting\n",
    "val1 = train_test_split(new_obj.iloc[:,:-1],new_obj.iloc[:,-1], train_size=0.8,random_state=0)\n",
    "# val1 = TimeSeriesSplit(n_splits=4, max_train_size=60_066)\n",
    "RF1 = RandomForestRegressor(n_estimators=300,n_jobs=20, random_state=0)\n",
    "RF1.fit(val1[0],val1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF1.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpv1(data: pd.DataFrame, y_true: np.ndarray[float], f: callable,*, n_boots: int, alpha: float, column_set: dict) -> tuple[pd.DataFrame]:\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = column_set\n",
    "    V_y = np.var(y_true, ddof=1)\n",
    "    u = dict()\n",
    "    CI = dict()\n",
    "    rolled_data = pd.DataFrame(np.roll(data.copy(),rows//2, axis=0), columns=data.columns)\n",
    "    # rolled_data = np.roll(data.copy(), rows//2, axis=0)\n",
    "    for col in npv_columns:\n",
    "        d = rolled_data.copy()\n",
    "        d[col] = data[col].to_numpy()\n",
    "        y_pred = f(d)\n",
    "        u[col] = 1-np.mean(np.square(y_true - y_pred))/(2*V_y)\n",
    "        S_CI = 1 - bootstrapp(y_true,y_pred, n_boots)/(2*V_y)\n",
    "        p0,p1 = np.quantile(S_CI, [alpha,1-alpha])\n",
    "        CI[col] = [p0,p1,p1-p0]\n",
    "    CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    CI.index.name = 'Interactions'\n",
    "    CI.columns.name = 'Confidence Interval Difference'\n",
    "    SI = pd.DataFrame(u.values(), index=u.keys(), columns=['$P_{i}$'])\n",
    "    return u,CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpv2(data: np.ndarray[float], y_true: np.ndarray[float], f: callable,*,S_I: np.ndarray[float] ,n_boots: int, alpha: float, columns_set: dict) -> tuple[np.ndarray[float], list]:\n",
    "    '''\n",
    "    ===Second-Order Permutation Variable Importances (Full Matrix)===\n",
    "\n",
    "        data: your training dataset\n",
    "        y_pred: your model output (specifically the y_true value)\n",
    "        f: in this case, simply RF.predict\n",
    "\n",
    "        S_I: First-Order Importances\n",
    "        n_boots: number of bootstrap samples\n",
    "        alpha: parameter for confidence interval [alpha, 1-alpha]\n",
    "        S_CI: sampling distribution \n",
    "        CI: Confidence Intervals\n",
    "    '''\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = columns_set\n",
    "    n = n_boots\n",
    "    S = dict()\n",
    "    CI = dict()\n",
    "    V_y = np.var(y_true, ddof=1)\n",
    "    rolled_data = pd.DataFrame(np.roll(data.copy(),rows//2, axis=0), columns=data.columns)\n",
    "    # rolled_data = np.roll(data.copy(), rows//2, axis=0)\n",
    "    for col1, col2 in it.combinations(npv_columns, r=2):\n",
    "        d = rolled_data.copy()\n",
    "        d[[col1,col2]] = data[[col1,col2]].to_numpy()\n",
    "        y_pred = f(d)\n",
    "        S_cat1 = S_I[col1]\n",
    "        S_cat2 = S_I[col2]\n",
    "        S[(col1, col2)] = 1 - np.mean(np.square(y_true - y_pred))/(2*V_y) - S_cat1 - S_cat2\n",
    "        S_CI = 1 - bootstrapp(y_true,y_pred, n_boots)/(2*V_y) - S_cat1 - S_cat2 # Bootstrap Process\n",
    "        p0,p1= np.quantile(S_CI, [alpha,1-alpha])\n",
    "        CI[(col1, col2)] = [p0,p1,p1-p0]\n",
    "    CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    CI.index.name = 'Interactions'\n",
    "    CI.columns.name = 'Confidence Interval Difference'\n",
    "\n",
    "    # S = pd.DataFrame(S.values(), index=S.keys(), columns=[r'$P_{ik}$'])\n",
    "    return S, CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI, CI = mpv1(val1[0], val1[2], RF1.predict, n_boots=1000, alpha=0.05, column_set=val1[0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT, CT = mpt(val1[0], val1[2], RF1.predict, n_boots=1000, alpha=0.05, column_set=val1[0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT = pd.Series(PT.values(), index=PT.keys())\n",
    "PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIK, CIK = mpv2(val1[0], val1[2], RF1.predict,S_I=PI, n_boots=1000, alpha=0.05, columns_set=val1[0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIK = pd.DataFrame(PIK.values(), index=PIK.keys(), columns=[r'$P_{ik}$'])\n",
    "PIK.to_excel(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\PVi_model_measures\\Second_Order_PVI.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features <= the importance of S_var\n",
    "feat_tb_dropped = PT[PT<=PT.loc['S_var']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_tb_dropped = set(feat_tb_dropped.index) - {'S_var'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(feat_tb_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've found the features contributing little to the predicition of the model output, we can remove them and re-train, validate, and test our the new RF model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reloading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\ensemble_output\\results\\default\\parameters.csv\")\n",
    "X.drop(columns=list(feat_tb_dropped), inplace=True)\n",
    "Y = pd.read_csv(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\ensemble_output\\results\\default\\gmslr.csv\")\n",
    "Y = Y.mean(axis=1).rename('output')\n",
    "obj = X.join(Y, how='left')\n",
    "obj.sort_values(by='t_peak',ascending=True, inplace=True)\n",
    "nun = len(X.t_peak.unique())\n",
    "discret = KBinsDiscretizer(n_bins=nun, encode='ordinal', random_state=0)\n",
    "Y_binned = discret.fit_transform(Y.to_numpy().reshape(-1,1)).ravel()\n",
    "smote = SMOTENC(categorical_features=['t_peak'],sampling_strategy='auto', random_state=0)\n",
    "X_res , y_res = smote.fit_resample(X, Y_binned)\n",
    "Y1 = pd.Series(discret.inverse_transform(y_res.reshape(-1,1)).ravel(), name='output')\n",
    "new_obj1 = X_res.join(Y1, how='left')\n",
    "new_obj1.sort_values(by='t_peak',ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obj1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Validation/Calibration No.1**\\\n",
    "As this is a time-series dataset, we define a new variable ```val2``` that partitions the dataset (w/o shuffling) into train/validation. When it's time to 'test' the performance of the model, we will switch to ```train_test_split```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val2 = train_test_split(obj.iloc[:,:-1],obj.iloc[:,-1],shuffle=False, train_size=0.8, random_state=0)\n",
    "val2 = TimeSeriesSplit(n_splits=4, max_train_size=60_066)\n",
    "RF = RandomForestRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Validation/Calibration No.2**\\\n",
    "Defining the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = dict()\n",
    "parameter_space['n_estimators'] = Integer(30,400)\n",
    "# parameter_space['min_samples_split'] = Integer(2,5, prior='uniform')\n",
    "# parameter_space['min_samples_leaf'] = Integer(1,5)\n",
    "parameter_space['max_features'] = Integer(5,22)\n",
    "parameter_space['max_depth'] = Integer(10,80)\n",
    "# parameter_space['ccp_alpha'] = Real(1e-4, 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bae = BayesSearchCV(estimator=RF,search_spaces=parameter_space,verbose=2, n_iter=100, scoring='neg_mean_squared_error', n_points=6, cv=val2, n_jobs=24, random_state=0)\n",
    "bae.fit(new_obj1.iloc[:,:-1], new_obj1.iloc[:,-1])\n",
    "res = pd.DataFrame(bae.cv_results_)\n",
    "res.to_excel(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\Hyperparameter_Tuning\\Darnell_HT_MSE_FS_3.xlsx\")\n",
    "res.to_csv(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\Hyperparameter_Tuning\\Darnell_HT_MSE_FS_3.csv\")\n",
    "print(f'Best Parameters:{bae.best_params_} with MSE: {bae.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2 = train_test_split(new_obj1.iloc[:,:-1],new_obj1.iloc[:,-1],shuffle=False, train_size=0.8, random_state=0)\n",
    "RF1 = RandomForestRegressor(n_estimators=400, max_features=18, max_depth=76, n_jobs=20, random_state=0)\n",
    "RF1.fit(val2[0], val2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variables = ['sd_temp', 'sd_ocean_heat', 'sd_glaciers', 'sd_greenland','sd_antarctic',\n",
    "              'sd_gmsl', 'rho_temperature','rho_ocean_heat', \n",
    "              'rho_glaciers', 'rho_greenland', 'rho_antarctic','rho_gmsl']\n",
    "Climate = ['CO2_0', 'N2O_0', 'temperature_0','ocean_heat_0','Q10', 'CO2_fertilization',\n",
    "           'CO2_diffusivity', 'heat_diffusivity', 'rf_scale_aerosol','climate_sensitivity']\n",
    "Glaciers = ['glaciers_v0','glaciers_s0','glaciers_beta0', 'glaciers_n']\n",
    "Thermals = ['thermal_s0', 'thermal_alpha']\n",
    "\n",
    "Greenland = ['greenland_a', 'greenland_b','greenland_alpha', 'greenland_beta']\n",
    "Antarctic = [ 'antarctic_gamma', 'antarctic_alpha','antarctic_mu', 'antarctic_nu',\n",
    "              'antarctic_precip0', 'antarctic_kappa','antarctic_flow0', 'antarctic_runoff_height0',\n",
    "                'antarctic_c','antarctic_bed_height0', 'antarctic_slope', 'antarctic_lambda','antarctic_temp_threshold',\n",
    "                'antarctic_s0','anto_alpha', 'anto_beta']\n",
    "variables = list(set(Variables) & set(val2[0].columns))\n",
    "climate = list(set(Climate) & set(val2[0].columns))\n",
    "thermals = list(set(Thermals) & set(val2[0].columns))\n",
    "greenland = list(set(Greenland) & set(val2[0].columns))\n",
    "antarctic = list(set(Antarctic) & set(val2[0].columns))\n",
    "npv_column_set = dict()\n",
    "npv_column_set['Variables'] = variables\n",
    "npv_column_set['Climate'] = climate\n",
    "# npv_column_set['Glaciers'] = Glaciers\n",
    "npv_column_set['Thermals'] = thermals\n",
    "npv_column_set['Greenland'] = greenland\n",
    "npv_column_set['Antarctic'] = antarctic\n",
    "npv_column_set['Emissions'] = ['gamma_g', 't_peak', 'gamma_d', 'lw_random_sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train MSE={np.mean(np.square(val2[2].to_numpy()-RF1.predict(val2[0].to_numpy())))}')\n",
    "print(f'Test MSE={np.mean(np.square(val2[3].to_numpy()-RF1.predict(val2[1].to_numpy())))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(val2[1].t_peak, val2[3], color='blue', label='True')\n",
    "plt.plot(val2[1].t_peak, RF1.predict(val2[1]), color='firebrick', label='Predicted')\n",
    "plt.title('Test Predictions vs. True Output')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI, CI = mod_pvi1(val2[0], val2[2], RF1.predict, n_boots=1000, alpha=0.05, column_set=npv_column_set)\n",
    "PIK, CIK = mod_pvi2(val2[0], val2[2], RF1.predict,S_I=PI, n_boots=1000, alpha=0.05, columns_set=npv_column_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = pd.DataFrame(PI.values(), index=PI.keys(), columns=[r'$P_{i}$'])\n",
    "PIK = pd.DataFrame(PIK.values(), index=PIK.keys(), columns=[r'$P_{ik}$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI_confide = PI.join(CI, how='left')\n",
    "PIK_confide = PIK.join(CIK, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PI_confide.to_latex(float_format='{:.4f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PIK_confide.to_latex(float_format='{:.4f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PI[PI>0].dropna()['$P_{i}$'].to_dict()\n",
    "pik = PIK.to_dict()['$P_{ik}$']\n",
    "ref_pik = {','.join(key): pik[key] for key in pik.keys()}\n",
    "merged_measures =  PI[PI>0].dropna()['$P_{i}$'].to_dict()|ref_pik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(merged_measures.values())\n",
    "labels = list(merged_measures.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT, CT = mod_pvi_t(val2[0], val2[2], RF1.predict, n_boots=1000, alpha=0.05, column_set=npv_column_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "values2 = list(PT.values())\n",
    "labels2 = list(PT.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donut_chart(values: np.ndarray, values1: np.ndarray, labels: list,labels1: list, props: dict, kw: dict):\n",
    "    fig, ax= plt.subplots(1,2, dpi=400, figsize=(20,25))\n",
    "    wedges0, text0 = ax[0].pie(values,wedgeprops=props['wedgeprops'],colors=props['colors'], startangle= props['startangle'], textprops = props['textprops'])\n",
    "    for i, p in enumerate(wedges0):\n",
    "        ang = (p.theta2-p.theta1)/2 + p.theta1\n",
    "        x,y = np.cos(np.deg2rad(ang)), np.sin(np.deg2rad(ang))\n",
    "        horizontalalignment = {-1:'right', 1:'left'}[int(np.sign(x))]\n",
    "        connectionstyle = f'angle, angleA=0, angleB={ang}'\n",
    "        kw['arrowprops'].update({'connectionstyle':connectionstyle})\n",
    "        ax[0].annotate(f'{values[i]: .1%}', xy=(x,y), xytext=(1.1*np.sign(x),1.2*y), horizontalalignment=horizontalalignment, **kw)\n",
    "    ax[0].set_title('SNEASY-BRICK: First & Second-Order PVi')\n",
    "    ax[0].legend(labels=labels,loc='center', title='PVi Indices:' )\n",
    "\n",
    "    wedges1, text1 = ax[1].pie(values1,wedgeprops=props['wedgeprops'],colors=props['colors'], startangle= props['startangle'], textprops = props['textprops'])\n",
    "    for i, p in enumerate(wedges1):\n",
    "        ang = (p.theta2-p.theta1)/2 + p.theta1\n",
    "        x,y = np.cos(np.deg2rad(ang)), np.sin(np.deg2rad(ang))\n",
    "        horizontalalignment = {-1:'right', 1:'left'}[int(np.sign(x))]\n",
    "        connectionstyle = f'angle, angleA=0, angleB={ang}'\n",
    "        kw['arrowprops'].update({'connectionstyle':connectionstyle})\n",
    "        ax[1].annotate(f'{values1[i]: .1%}', xy=(x,y), xytext=(1.1*np.sign(x),1.2*y), horizontalalignment=horizontalalignment, **kw)\n",
    "    ax[1].set_title('SNEASY-BRICK: Total-Order PVi')\n",
    "    ax[1].legend(labels=labels1,loc='center', title='PVi Indices:' )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donut_chart(values, values2, labels, labels2, props, kw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
