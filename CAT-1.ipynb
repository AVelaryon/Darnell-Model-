{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "import scipy.stats.qmc as ssq\n",
    "from scipy.stats import chi2, truncnorm\n",
    "from seaborn import displot\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space.space import Real, Integer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer\n",
    "\n",
    "np.set_printoptions(threshold=100_000)\n",
    "X = pd.read_csv(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\ensemble_output\\results\\default\\parameters.csv\")\n",
    "# X = X[['gamma_g', 't_peak', 'gamma_d', 'sd_antarctic',\n",
    "#        'rho_greenland', 'rho_gmsl', 'temperature_0', 'ocean_heat_0', 'Q10',\n",
    "#        'CO2_diffusivity', 'heat_diffusivity', 'rf_scale_aerosol',\n",
    "#        'climate_sensitivity', 'thermal_alpha', 'greenland_a', 'greenland_b',\n",
    "#        'greenland_alpha', 'greenland_beta', 'anto_alpha', 'antarctic_mu',\n",
    "#        'antarctic_precip0', 'antarctic_runoff_height0', 'antarctic_lambda',\n",
    "#        'antarctic_temp_threshold', 'lw_random_sample']].copy()\n",
    "Y = pd.read_csv(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\ensemble_output\\results\\default\\gmslr.csv\")\n",
    "Y = Y.mean(axis=1).rename('output')\n",
    "sorted_indices = X.t_peak.to_numpy().ravel().argsort()\n",
    "# bins = pd.IntervalIndex.from_tuples([(2030, 2069), (2070,2109), (2110, 2149), (2150, 2178)])\n",
    "# X['t_peak'] = pd.cut(X.t_peak.to_numpy(), bins=4)\n",
    "X.t_peak = X.t_peak.astype('category')\n",
    "X = pd.get_dummies(X,prefix=['emission_year'], dtype=float)\n",
    "obj = X.join(Y, how='left')\n",
    "sorted_obj = obj.iloc[sorted_indices,:].copy()\n",
    "# Rescaling the dataset\n",
    "# min_max = MinMaxScaler()\n",
    "# min_max.fit(obj.iloc[:,:-1],obj.iloc[:,-1])\n",
    "\n",
    "# obj_scaled = min_max.transform(obj.iloc[:,:-1])\n",
    "# obj_y = obj.iloc[:,-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_obj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = train_test_split(sorted_obj.iloc[:,:-1], sorted_obj.iloc[:,-1],shuffle=False, train_size=0.8, random_state=0)\n",
    "val = TimeSeriesSplit(n_splits=4, max_train_size=60_000)\n",
    "RF = RandomForestRegressor(random_state=0)\n",
    "# RF.fit(val[0].to_numpy(), val[2].to_numpy())\n",
    "# print(f'Training MSE:\\t{np.square(val[2].to_numpy()-RF.predict(val[0].to_numpy())).mean()}')\n",
    "# print(f'Testing MSE:\\t{np.square(val[3].to_numpy()-RF.predict(val[1].to_numpy())).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = dict()\n",
    "parameter_space['n_estimators'] = Integer(30,300)\n",
    "# parameter_space['min_samples_split'] = Integer(2,5, prior='uniform')\n",
    "# parameter_space['min_samples_leaf'] = Integer(1,5)\n",
    "parameter_space['max_features'] = Integer(10,197)\n",
    "parameter_space['max_depth'] = Integer(10,70)\n",
    "# parameter_space['ccp_alpha'] = Real(1e-4, 5e-4)\n",
    "# 'max_depth': 26, 'max_features': 19, 'n_estimators': 300}) with MSE: -0.2149719486225359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "Best Parameters:OrderedDict({'max_depth': 26, 'max_features': 19, 'n_estimators': 300}) with MSE: -0.2149719486225359\n"
     ]
    }
   ],
   "source": [
    "bae = BayesSearchCV(estimator=RF,search_spaces=parameter_space,verbose=10, n_iter=100, scoring='neg_mean_squared_error', n_points=6, cv=val, n_jobs=24, random_state=0)\n",
    "bae.fit(sorted_obj.iloc[:,:-1], sorted_obj.iloc[:,-1])\n",
    "res = pd.DataFrame(bae.cv_results_)\n",
    "res.to_excel(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\Hyperparameter_Tuning\\CAT_HT.xlsx\")\n",
    "res.to_csv(r\"C:\\Users\\dwigh\\OneDrive\\Desktop\\Emissions Uncertainty on Antarctic Instability\\Hyperparameter_Tuning\\CAT_HT.csv\")\n",
    "print(f'Best Parameters:{bae.best_params_} with MSE: {bae.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=0)\n",
    "def bootstrapp(y_true,y_pred,n_boots):\n",
    "    rows = len(y_true)\n",
    "    error = (y_true - y_pred).to_numpy()\n",
    "    S_CI = np.zeros((n_boots,))\n",
    "    for i in range(n_boots):\n",
    "        indx = rng.choice(rows, size=rows, replace=True)\n",
    "        S_CI[i] = np.mean(np.square(error[indx]))\n",
    "    return S_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvi1(data: pd.DataFrame, y_true: np.ndarray[float], f: callable,*, n_boots: int, alpha: float, column_set: dict) -> tuple[pd.DataFrame]:\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = column_set\n",
    "    V_y = np.var(y_true, ddof=1)\n",
    "    u = dict()\n",
    "    CI = dict()\n",
    "    # rolled_data = pd.DataFrame(np.roll(data.copy(),rows//2, axis=0), columns=data.columns)\n",
    "    rolled_data = np.roll(data.copy(),rows//2, axis=0)\n",
    "    # for cat,col in npv_columns.items():\n",
    "    for col, cat in enumerate(npv_columns):\n",
    "        d = rolled_data.copy()\n",
    "        d[:, col] = data[:,col]\n",
    "        y_pred = f(d)\n",
    "        u[cat] = 1-np.mean(np.square(y_true - y_pred))/(2*V_y)\n",
    "    #     S_CI = 1 - bootstrapp(y_true,y_pred, n_boots)/(2*V_y)\n",
    "    #     p0,p1 = np.quantile(S_CI, [alpha,1-alpha])\n",
    "    #     CI[col] = [p0,p1,p1-p0]\n",
    "    # CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    # CI.index.name = 'Interactions'\n",
    "    # CI.columns.name = 'Confidence Interval Difference'\n",
    "    # SI = pd.DataFrame(u.values(), index=u.keys(), columns=['$P_{i}$'])\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvi2(data: np.ndarray[float], y_true: np.ndarray[float], f: callable,*,S_I: np.ndarray[float] ,n_boots: int, alpha: float, columns_set: dict) -> tuple[np.ndarray[float], list]:\n",
    "    '''\n",
    "    ===Second-Order Permutation Variable Importances (Full Matrix)===\n",
    "\n",
    "        data: your training dataset\n",
    "        y_pred: your model output (specifically the y_true value)\n",
    "        f: in this case, simply RF.predict\n",
    "\n",
    "        S_I: First-Order Importances\n",
    "        n_boots: number of bootstrap samples\n",
    "        alpha: parameter for confidence interval [alpha, 1-alpha]\n",
    "        S_CI: sampling distribution \n",
    "        CI: Confidence Intervals\n",
    "    '''\n",
    "    rows, columns = data.shape\n",
    "    npv_columns = columns_set\n",
    "    n = n_boots\n",
    "    S = dict()\n",
    "    CI = dict()\n",
    "    V_y = np.var(y_true, ddof=1)\n",
    "    # rolled_data = pd.DataFrame(np.roll(data.copy(),rows//2, axis=0), columns=data.columns)\n",
    "    rolled_data = np.roll(data.copy(),rows//2, axis=0)\n",
    "    for (cat1,col1),(cat2,col2) in it.combinations(npv_columns, r=2):\n",
    "        d = rolled_data.copy()\n",
    "        d[:,[cat1, cat2]] = data[:,[cat1,cat2]]\n",
    "        y_pred = f(d)\n",
    "        S_cat1 = S_I[col1]\n",
    "        S_cat2 = S_I[col2]\n",
    "        # print(f'First-Order:\\t{S_cat1, S_cat2}')\n",
    "        # print(f'First Part:\\t{np.mean(np.square(y_true - f(d)))/(2*V_y)}')\n",
    "        # print(f'Second Part:\\t{S_cat1}')\n",
    "        # print(f'Third Part:\\t{S_cat2}')\n",
    "        S[(col1,col2)] = 1 - np.mean(np.square(y_true - y_pred))/(2*V_y) - S_cat1 - S_cat2\n",
    "        print(f'Second-Order:{(col1,col2)}\\t{S[col1,col2]}')\n",
    "\n",
    "    #     S_CI = 1 - bootstrapp(y_true,y_pred, n_boots)/(2*V_y) - S_cat1 - S_cat2\n",
    "    #     p0,p1= np.quantile(S_CI, [alpha,1-alpha])\n",
    "    #     CI[(col1, col2)] = [p0,p1,p1-p0]\n",
    "    # CI = pd.DataFrame(CI.values(), index = CI.keys(), columns=['5th','95th','Quantile Difference'])\n",
    "    # CI.index.name = 'Interactions'\n",
    "    # CI.columns.name = 'Confidence Interval Difference'\n",
    "\n",
    "    # S = pd.DataFrame(S.values(), index=S.keys(), columns=[r'$P_{ik}$'])\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = pvi1(val[0].to_numpy(), val[2].to_numpy(), RF.predict,n_boots=None, alpha=None, column_set=val[0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(range(val[0].shape[1]), val[0].columns)\n",
    "# print(list(zipped))\n",
    "PIK = pvi2(val[0].to_numpy(), val[2].to_numpy(), RF.predict, S_I=PI,n_boots=None, alpha=None, columns_set=list(zipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(range(val[0].shape[1]), val[0].columns)\n",
    "print(list(zipped))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
